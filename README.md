# ðŸ” Smart HTML Extraction Pipeline

**Intelligent web scraping with AI-powered analysis and fingerprint caching**

## ðŸŽ¯ What This Does

Drop an HTML file into a folder â†’ Get structured data extracted automatically

The system:
1. **Cleans** the HTML (removes Base64, CSS, JavaScript bloat)
2. **Analyzes** the page (determines domain, page type, what data to extract)
3. **Checks cache** (reuses extraction plans for similar pages = big cost savings)
4. **Extracts** the data (uses JSON-LD and CSS selectors)
5. **Verifies** quality (AI reviews completeness)
6. **Reports** results (Markdown + JSON + Debug files)

## ðŸ’° Cost Structure

### First-Time Page Structure
- Context Analysis: ~$0.03-0.05 (GPT-4o)
- Technical Planning: ~$0.003 (GPT-4o-mini)
- Verification: ~$0.03-0.05 (GPT-4o)
- **Total: ~$0.12-0.17 per page**

### Cached Page Structure (2nd+ time)
- Extraction: Free (Python)
- Verification: ~$0.03-0.05 (GPT-4o)
- **Total: ~$0.05 per page**

### Real-World Examples

**Researching 50 eBay sold listings:**
- Page 1: $0.17 (learns the structure)
- Pages 2-50: 49 Ã— $0.05 = $2.45
- **Total: $2.62 ($0.05/page average)**

**Checking prices across 3 different sites:**
- 10 Schiit pages: $0.17 + 9Ã—$0.05 = $0.62
- 10 Emotiva pages: $0.17 + 9Ã—$0.05 = $0.62
- 20 Crutchfield pages: $0.17 + 19Ã—$0.05 = $1.12
- **Total: $2.36 for 40 pages**

## ðŸ“ Directory Structure

```
your_project/
â”œâ”€â”€ Pi_Inbox/
â”‚   â”œâ”€â”€ Research_Queue/       â† DROP HTML FILES HERE
â”‚   â”œâ”€â”€ Processed_Archive/    â† Successfully processed files
â”‚   â”œâ”€â”€ Errors/                â† Failed processing files
â”‚   â””â”€â”€ Output/                â† Reports (MD + JSON + DEBUG)
â”œâ”€â”€ config.py
â”œâ”€â”€ main_runner.py
â”œâ”€â”€ fingerprint.py             â† Cache management
â”œâ”€â”€ html_brief.py              â† HTML cleaning
â”œâ”€â”€ planner.py                 â† AI analysis
â”œâ”€â”€ extractor.py               â† Data extraction
â”œâ”€â”€ verifier.py                â† Quality check
â”œâ”€â”€ reporter.py                â† Report generation
â”œâ”€â”€ utils_logging.py
â”œâ”€â”€ .env                       â† Your API key
â”œâ”€â”€ research_pipeline.log      â† Activity log
â”œâ”€â”€ token_usage_log.csv        â† Cost tracking
â””â”€â”€ fingerprint_cache.json     â† Saved extraction plans
```

## ðŸš€ Setup

### 1. Install Dependencies

```bash
pip install openai beautifulsoup4 lxml jsonpath-ng watchdog python-dotenv --break-system-packages
```

### 2. Configure API Key

Create a `.env` file:

```
OPENAI_API_KEY=sk-your-key-here
```

### 3. Create Directories

```bash
mkdir -p Pi_Inbox/Research_Queue
mkdir -p Pi_Inbox/Processed_Archive
mkdir -p Pi_Inbox/Errors
mkdir -p Pi_Inbox/Output
```

### 4. Start the Pipeline

```bash
python main_runner.py
```

You'll see:
```
ðŸš€ SMART EXTRACTION PIPELINE - STARTED
ðŸ“‚ Monitoring: /path/to/Pi_Inbox/Research_Queue
ðŸ’¾ Cache file: /path/to/fingerprint_cache.json
ðŸ§  Smart Model: gpt-4o
âš¡ Fast Model: gpt-4o-mini

ðŸ“Š Cache Statistics:
   Cached Plans: 0
   Total Uses: 0
   Avg Success: 0.0%

â³ Waiting for HTML files...
```

## ðŸ“¥ How to Use

### Save Web Pages as HTML

**Chrome/Safari:**
1. Visit the page (eBay listing, product page, etc.)
2. Right-click â†’ "Save As..."
3. Format: "Webpage, Complete" or "Web Archive"
4. Save to: `Pi_Inbox/Research_Queue/`

**Or use browser extensions:**
- SingleFile (Chrome/Firefox) - captures everything in one HTML file
- Save Page WE (Firefox)

### Watch It Process

The system automatically detects new files and processes them:

```
ðŸ“¥ New file detected: ebay_nikon_d850.html
ðŸš€ Processing: ebay_nikon_d850.html

ðŸ“‹ STEP A: Creating HTML Brief...
   Found 1 JSON-LD blobs
   Removed 47 non-content tags
   Brief complete: 1247.3KB â†’ 45.2KB (96.4% reduction)
   Estimated tokens: ~11,300

ðŸ” STEP B: Checking Fingerprint Cache...
   Generated fingerprint: 3f7a9c2e1b4d8f6a (ebay.com|ItemList|s-card|price:True|title:True)
   âš ï¸ Cache miss - performing full analysis

ðŸ§  STEP C: Context Analysis...
   Domain: Used Camera Equipment Marketplace
   Page Type: Search Results
   ðŸ’° Cost: $0.0328 (13,142 in, 127 out)

ðŸ”§ STEP D: Technical Planning...
   Created 8 extraction strategies
   ðŸ’° Cost: $0.0024 (13,890 in, 243 out)

âš™ï¸ STEP E: Executing Extraction...
   Processing 8 extraction strategies...
   Extraction complete: 7/8 successful (87.5%)

âœ… STEP F: Verification & Summary...
   Completeness: 87
   ðŸ’° Cost: $0.0187 (1,847 in, 156 out)

ðŸ“Š STEP G: Generating Report...
   ðŸ“„ Reports generated:
      - 20241225-1430_ebay_nikon_d850.md
      - 20241225-1430_ebay_nikon_d850.json
      - 20241225-1430_ebay_nikon_d850_DEBUG.txt

âœ… SUCCESS (Full Analysis) - Archived: ebay_nikon_d850.html
```

### Check Your Results

**Markdown Report** (human-readable):
- Executive summary
- Cache hit status (saved costs)
- Extracted data table
- Missing fields warnings

**JSON File** (for scripts/databases):
- Complete structured data
- Extraction plan used
- All metadata

**Debug File** (troubleshooting):
- Every selector tried
- What worked, what didn't
- Detailed extraction steps

## ðŸ“Š Cost Tracking

All API calls are logged to `token_usage_log.csv`:

```csv
Timestamp,Task,Model,Input_Tokens,Output_Tokens,Cost_USD
2024-12-25T14:30:15,Context Analysis,gpt-4o,13142,127,$0.033280
2024-12-25T14:30:18,Technical Planning,gpt-4o-mini,13890,243,$0.002229
2024-12-25T14:30:22,Verification,gpt-4o,1847,156,$0.018695
```

Import into Excel/Google Sheets to analyze spending patterns.

## ðŸŽ¯ Optimizing Costs

### Do This:
âœ… Process similar pages together (eBay sold listings, Reverb gear pages, etc.)
âœ… Let the cache build up (2nd+ pages are 70% cheaper)
âœ… Monitor `fingerprint_cache.json` to see what's cached

### Don't Do This:
âŒ Process one-off random pages (no cache benefit)
âŒ Delete `fingerprint_cache.json` (you lose all savings)
âŒ Mix completely different page types in one batch

### Cache Benefits

The more you use the same sites, the cheaper it gets:

| Pages Processed | Average Cost |
|-----------------|--------------|
| 1 page (new structure) | $0.17 |
| 5 pages (same structure) | $0.08 |
| 10 pages (same structure) | $0.06 |
| 50 pages (same structure) | $0.05 |
| 100 pages (same structure) | $0.05 |

## ðŸ› ï¸ Configuration Options

Edit `config.py` to adjust:

```python
# Model selection
MODEL_SMART = "gpt-4o"       # For analysis & verification
MODEL_FAST = "gpt-4o-mini"   # For technical planning

# Cache settings
CACHE_MIN_SUCCESS_RATE = 0.85  # Only use plans with >85% success
CACHE_STALENESS_DAYS = 30      # Re-analyze after 30 days
```

## ðŸ› Troubleshooting

### "Context length exceeded" error
- Your HTML is >500KB after cleaning
- Solution: The system should handle this, but if it fails, try a simpler page

### Low completeness scores
- Check `*_DEBUG.txt` file to see what selectors failed
- Website might have changed structure
- Delete the fingerprint entry to force re-analysis

### Cache not working
- Fingerprint might be too specific
- Check `fingerprint_cache.json` to see stored plans
- Look for similar entries that should match but don't

## ðŸ“ˆ What Pages Work Best

**Excellent (95%+ accuracy):**
- eBay sold listings
- Amazon product pages
- Brand manufacturer sites (Schiit, Emotiva, etc.)
- Craigslist listings

**Good (85-95% accuracy):**
- Retailer category pages (Micro Center, Crutchfield)
- Multi-vendor marketplaces
- Specification pages

**Fair (70-85% accuracy):**
- Review sites (complex comparison tables)
- Forum posts (threaded discussions)
- Dynamic JavaScript-heavy sites

## ðŸŽ“ Advanced Usage

### Batch Processing

Drop 50 files at once:
```bash
cp ~/Downloads/ebay_listings/*.html Pi_Inbox/Research_Queue/
```

The system processes them sequentially, building cache as it goes.

### Integration with Other Tools

The JSON output is perfect for:
- Python scripts (pandas, data analysis)
- Spreadsheets (import JSON)
- Databases (insert structured data)
- Price tracking systems

Example Python usage:
```python
import json

with open('Pi_Inbox/Output/20241225-1430_ebay_camera.json') as f:
    data = json.load(f)
    
price = data['extracted_data']['price']
condition = data['extracted_data']['condition']
print(f"Found: {condition} for {price}")
```

## ðŸ’¡ Tips & Tricks

1. **Name your HTML files descriptively**
   - Good: `ebay_nikon_d850_sold_dec2024.html`
   - Bad: `page.html`

2. **Create folders for different research projects**
   - Organize cache by keeping related research together
   - Delete cache when switching to completely different domains

3. **Check completeness scores**
   - >90% = excellent extraction
   - 80-90% = good, minor fields missing
   - <80% = review debug file, might need re-analysis

4. **Review the first extraction carefully**
   - It creates the template for all future similar pages
   - If it's wrong, delete the cache entry and try again

## ðŸ“ž Questions?

Check the logs:
- `research_pipeline.log` - detailed activity log
- `token_usage_log.csv` - cost tracking
- `*_DEBUG.txt` - per-file extraction details

---

**Built for accuracy. Optimized for cost. Designed for scale.**